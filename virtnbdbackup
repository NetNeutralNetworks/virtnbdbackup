#!/usr/bin/python3
"""
    Copyright (C) 2021  Michael Ablassmeier <abi@grinser.de>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""
import os
import sys
import json
import signal
import logging
import argparse
from time import time
from datetime import datetime
from functools import partial
from threading import current_thread
from concurrent.futures import ThreadPoolExecutor, as_completed

from libvirtnbdbackup import __version__
from libvirtnbdbackup import common
from libvirtnbdbackup import nbdhelper
from libvirtnbdbackup import extenthandler
from libvirtnbdbackup import qemuhelper
from libvirtnbdbackup import libvirthelper
from libvirtnbdbackup import outputhelper
from libvirtnbdbackup import exceptions
from libvirtnbdbackup.sparsestream import streamer
from libvirtnbdbackup.sparsestream import types


class logCount(logging.Handler):
    """Custom log handler keeping track of issued log messages"""

    class LogType:
        """Log message type"""

        def __init__(self):
            self.warnings = 0
            self.errors = 0

    def __init__(self):
        super().__init__()
        self.count = self.LogType()

    def emit(self, record):
        if record.levelname == "WARNING":
            self.count.warnings += 1
        if record.levelname == "ERROR":
            self.count.errors += 1


def handleSignal(args, domObj, virtClient, log, signum, _):
    """Catch signal, attempt to stop running backup job."""
    log.error("Caught signal: %s", signum)
    log.error("Cleanup: Stopping backup job")
    if args.offline is not True:
        virtClient.stopBackup(domObj)
    sys.exit(1)


def checkForeign(args, virtClient, domObj, lib):
    """Check and warn user if virtual machine has checkpoints
    not originating from this utility"""
    foreign = None
    if args.level in ("full", "inc", "diff"):
        foreign = virtClient.hasforeignCheckpoint(domObj, lib.checkpointName)

    if not foreign:
        return True

    logging.fatal("VM has external checkpoint: [%s]", foreign)
    logging.fatal("This checkpoint has not been created by this utility.")
    logging.fatal(
        "To ensure checkpoint chain consistency,"
        "remove existing checkpoints,"
        "and start a new backup chain by creating a full backup."
    )

    raise exceptions.ForeignCeckpointError()


def setOfflineArguments(args, virtClient, domObj):
    """Check if to be saved VM is offline and set
    propper options/overwrite backup mode"""
    args.offline = False
    if virtClient.domainOffline(domObj) is True:
        if args.level == "full":
            logging.warning("Domain is offline, resetting backup options")
            args.level = "copy"
            logging.info("Backup level: [%s]", args.level)
        args.offline = True


def hasPartial(args, lib):
    """Check if target directory has an partial backup,
    makes backup utility exit errnous in case backup
    type is full or inc"""
    if (
        args.level in ("inc", "diff")
        and args.stdout is False
        and lib.partialBackup(args) is True
    ):
        logging.error("Partial backup found in target directory: %s", args.output)
        logging.error("One of the last backups seems to have failed.")
        logging.error("Consider re-executing full backup.")
        return True

    return False


def readCheckpointFile(cptFile):
    """Open checkpoint file and read checkpoint
    information"""
    checkpoints = []
    if os.path.exists(cptFile):
        with open(cptFile, "r") as cptFh:
            checkpoints = json.loads(cptFh.read())

    return checkpoints


def handleCheckpoints(args, virtClient, domObj, lib):
    """Checkpoint handling for different backup modes
    to be executed"""
    checkpointName = f"{lib.checkpointName}.0"
    parentCheckpoint = False
    cptFile = f"{args.output}/{args.domain}.cpt"
    checkpoints = readCheckpointFile(cptFile)

    if args.offline is False:
        if virtClient.redefineCheckpoints(domObj, args) is False:
            raise exceptions.RedefineCheckpointError("Unable to redefine checkpoints.")

    logging.info("Checkpoint handling")
    if args.level == "full" and checkpoints:
        logging.info("Removing all existent checkpoints before full backup")
        virtClient.removeAllCheckpoints(domObj, checkpoints, args, lib.checkpointName)
        os.remove(cptFile)
        checkpoints = []
    elif args.level == "full" and len(checkpoints) < 1:
        virtClient.removeAllCheckpoints(domObj, None, args, lib.checkpointName)
        checkpoints = []

    if checkpoints and args.level in ("inc", "diff"):
        nextCpt = len(checkpoints)
        checkpointName = f"{lib.checkpointName}.{nextCpt}"
        parentCheckpoint = checkpoints[-1]
        logging.info("Next checkpoint id: [%s]", nextCpt)
        logging.info("Parent checkpoint name [%s]", parentCheckpoint)

        if args.offline is True:
            logging.info("Offline backup, use last known checkpoint, save only delta.")
            checkpointName = parentCheckpoint

    if args.level == "diff":
        logging.info(
            "Diff backup: saving delta since checkpoint: [%s]", parentCheckpoint
        )

    if args.level in ("inc", "diff") and len(checkpoints) < 1:
        raise exceptions.NoCheckpointsFound(
            "No existing checkpoints found, execute full backup first."
        )

    if args.level in ("full", "inc"):
        logging.info("Using checkpoint name: %s", checkpointName)

    return checkpointName, parentCheckpoint, cptFile


def saveCheckpointFile(cptFile, checkpointName):
    """Append created checkpoint to checkpoint
    file"""
    try:
        checkpoints = readCheckpointFile(cptFile)
        checkpoints.append(checkpointName)
        with open(cptFile, "w") as cFw:
            cFw.write(json.dumps(checkpoints))
    except exceptions.CheckpointException() as e:
        raise exceptions.CheckpointException from e
    except OSError as e:
        raise exceptions.SaveCheckpointError from e


def main():
    """Handle backup operation"""
    parser = argparse.ArgumentParser(description="Backup libvirt/qemu virtual machines")
    parser.add_argument(
        "-d", "--domain", required=True, type=str, help="Domain to backup"
    )
    parser.add_argument(
        "-l",
        "--level",
        default="copy",
        choices=["copy", "full", "inc", "diff"],
        type=str,
        help="Backup level. (default: %(default)s)",
    )
    parser.add_argument(
        "-t",
        "--type",
        default="stream",
        type=str,
        choices=["stream", "raw"],
        help="Output type: stream or raw. (default: %(default)s)",
    )
    parser.add_argument(
        "-r",
        "--raw",
        default=False,
        action="store_true",
        help="Include full provisioned disk images in backup. (default: %(default)s)",
    )
    parser.add_argument(
        "-o", "--output", required=True, type=str, help="Output target directory"
    )
    parser.add_argument(
        "-C",
        "--checkpointdir",
        required=False,
        default=None,
        type=str,
        help="Persistent libvirt checkpoint storage directory",
    )
    parser.add_argument(
        "-S",
        "--scratchdir",
        default="/var/tmp",
        required=False,
        type=str,
        help="Target directory for temporary scratch file. (default: %(default)s)",
    )
    parser.add_argument(
        "-q",
        "--qemu",
        default=False,
        action="store_true",
        help="Use Qemu tools to query extents (full and copy only)",
    )
    parser.add_argument(
        "-i",
        "--include",
        default=None,
        type=str,
        help="Backup only disk with target dev name (-i vda)",
    )
    parser.add_argument(
        "-x",
        "--exclude",
        default=None,
        type=str,
        help="Exclude disk(s) with target dev name (-x vda,vdb)",
    )
    parser.add_argument(
        "-s",
        "--startonly",
        default=False,
        help="Only initialize backup job via libvirt, do not backup any data",
        action="store_true",
    )
    parser.add_argument(
        "-k",
        "--killonly",
        default=False,
        help="Kill any running block job",
        action="store_true",
    )
    parser.add_argument(
        "-p",
        "--printonly",
        default=False,
        help="Quit after printing extent information",
        action="store_true",
    )
    parser.add_argument(
        "-f",
        "--socketfile",
        default=None,
        type=str,
        help="Use specified file for NBD Server socket instead of random file",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        default=False,
        help="Enable debug output",
        action="store_true",
    )
    parser.add_argument(
        "-n",
        "--noprogress",
        default=False,
        help="Disable progress bar",
        action="store_true",
    )
    parser.add_argument(
        "-z",
        "--compress",
        default=False,
        help="Compress with lz4. (default: %(default)s)",
        action="store_true",
    )
    parser.add_argument(
        "-w",
        "--worker",
        type=int,
        default=None,
        help=(
            "Amount of concurrent workers used"
            "to backup multiple disks. (default: amount of disks)"
        ),
    )
    parser.add_argument(
        "-e",
        "--strict",
        default=False,
        help=(
            "Change exit code if warnings occur" "during backup. (default: %(default)s)"
        ),
        action="store_true",
    )

    outHelper = outputhelper.outputHelper()
    lib = common.Common()
    args = lib.argparse(parser)

    args.stdout = args.output == "-"

    zipStream = None
    if args.stdout is False:
        outHelper.Directory(args.output)
    else:
        zipStream = outHelper.Zip()
        args.output = "./"
        args.worker = 1
        args.raw = False

    if args.worker is not None and args.worker < 1:
        args.worker = 1

    now = datetime.now().strftime("%m%d%Y%H%M%S")
    logFile = f"{args.output}/backup.{args.level}.{now}.log"

    counter = logCount()
    logging.basicConfig(
        level=lib.setLogLevel(args.verbose),
        format=lib.logFormat,
        datefmt=lib.logDateFormat,
        handlers=[
            logging.FileHandler(logFile),
            logging.StreamHandler(stream=sys.stderr),
            counter,
        ],
    )

    lib.printVersion(__version__)

    logging.info("Backup level: [%s]", args.level)
    if args.compress:
        logging.info("Compression enabled")

    if args.stdout is True and args.type == "raw":
        logging.error("Output type raw not supported to stdout")
        sys.exit(1)

    if not args.stdout and not args.startonly and not args.killonly:
        if not lib.targetIsEmpty(args):
            logging.error("Target directory must empty for full or copy backup.")
            sys.exit(1)

    if args.raw and args.level in ("inc", "diff"):
        logging.error(
            "Raw disks cant be included during incremental or differencial backup."
        )
        sys.exit(1)

    if args.type == "raw" and args.level in ("inc", "diff"):
        logging.error(
            "Backup format raw does not support incremental or differencial backup"
        )
        sys.exit(1)

    if args.printonly is True:
        logging.info("Printing only extend information: enforce level copy")
        args.level = "copy"

    if hasPartial(args, lib):
        sys.exit(1)

    if not args.checkpointdir:
        args.checkpointdir = f"{args.output}/checkpoints"
    else:
        logging.info("Store checkpoints in: %s", args.checkpointdir)

    outHelper.Directory(args.checkpointdir)

    virtClient = libvirthelper.client()
    try:
        domObj = virtClient.getDomain(args.domain)
    except Exception as e:
        logging.error("Unable to get domain information: %s", e)
        sys.exit(1)

    if virtClient.hasIncrementalEnabled(domObj) is False:
        logging.error("Domain is missing required incremental-backup capability.")
        sys.exit(1)

    try:
        checkForeign(args, virtClient, domObj, lib)
    except exceptions.CheckpointException():
        sys.exit(1)

    setOfflineArguments(args, virtClient, domObj)

    signal.signal(
        signal.SIGINT, partial(handleSignal, args, domObj, virtClient, logging)
    )

    vmConfig = virtClient.getDomainConfig(domObj)
    disks = virtClient.getDomainDisks(vmConfig, args.exclude, args.include, args.raw)

    if not disks:
        logging.error(
            "Domain has no disks attached which support changed block tracking."
        )
        sys.exit(1)

    logging.info(
        "Domain has %s disks attached which support changed block tracking.", len(disks)
    )
    if args.worker is None:
        args.worker = int(len(disks))
    logging.info("Concurrent backup processes: [%s]", args.worker)

    if args.killonly is True:
        logging.info("Stopping backup job")
        if not virtClient.stopBackup(domObj):
            sys.exit(1)
        sys.exit(0)

    try:
        checkpointName, parentCheckpoint, cptFile = handleCheckpoints(
            args, virtClient, domObj, lib
        )
    except exceptions.CheckpointException as errmsg:
        logging.error(errmsg)
        sys.exit(1)

    logging.info("Temporary scratch file target directory: %s", args.scratchdir)

    outHelper.Directory(args.scratchdir)

    backupSocket = lib.getSocketFile(args.socketfile)
    logging.info("NDB Endpoint socket: %s", backupSocket)

    if args.offline is not True:
        try:
            logging.info("Starting backup job.")
            virtClient.startBackup(
                domObj,
                disks,
                args.level,
                checkpointName,
                parentCheckpoint,
                args.scratchdir,
                backupSocket,
            )
            logging.debug("Backup job started, using socket: %s", backupSocket)
        except Exception as e:
            logging.error(e)
            sys.exit(1)

    if args.level not in ("copy", "diff") and args.offline is False:
        logging.info("Started backup job with checkpoint, saving information.")
        try:
            saveCheckpointFile(cptFile, checkpointName)
        except exceptions.CheckpointException() as e:
            logging.error("Unable to append checkpoint file: %s", e)
            sys.exit(1)
        if args.printonly is False:
            if not virtClient.backupCheckpoint(domObj, args, checkpointName):
                virtClient.stopBackup(domObj)
                sys.exit(1)

    if args.startonly is True:
        logging.info("Started backup job for debugging, exiting.")
        sys.exit(0)

    error = False
    try:
        with ThreadPoolExecutor(max_workers=args.worker) as executor:
            futures = {
                executor.submit(
                    backupDisk,
                    disk,
                    count,
                    args,
                    lib,
                    checkpointName,
                    parentCheckpoint,
                    backupSocket,
                    zipStream,
                ): disk
                for count, disk in enumerate(disks)
            }
            for future in as_completed(futures):
                if future.result() is not True:
                    raise exceptions.DiskBackupFailed("Backup of one disk failed")
    except exceptions.BackupException as e:
        logging.error("Unable to backup Disk: %s", e)
        logging.exception(e)
        error = True
    except Exception as e:
        logging.fatal("Unknown Exception during backup: %s", e)
        logging.exception(e)
        error = True

    configFile = None
    if args.printonly is False:
        configFile = backupConfig(vmConfig, args, checkpointName)

    if args.offline is False:
        logging.info("Backup jobs finished, stopping backup task.")
        virtClient.stopBackup(domObj)

    if args.stdout is True and args.printonly is False:
        addFiles(configFile, zipStream, cptFile, logFile, args)

    if error is True:
        sys.exit(1)

    if counter.count.warnings > 0 and args.strict is True:
        logging.info(
            "[%s] Warnings detected during backup operation, forcing exit code 2",
            counter.count.warnings,
        )
        sys.exit(2)

    logging.info("Finished successfully")


def addFiles(configFile, zipStream, cptFile, logFile, args):
    """Add backup log and other files to zip archive"""
    if configFile is not None:
        logging.info("Adding vm config to zipfile")
        zipStream.zipStream.write(configFile, configFile)
    if args.level in ("full", "inc"):
        logging.info("Adding checkpoint info to zipfile")
        zipStream.zipStream.write(cptFile, cptFile)
        for dirname, _, files in os.walk(args.checkpointdir):
            zipStream.zipStream.write(dirname)
            for filename in files:
                zipStream.zipStream.write(os.path.join(dirname, filename))
    logging.info("Adding backup log to zipfile")
    zipStream.zipStream.write(logFile, logFile)


def backupConfig(vmConfig, args, checkpointName):
    """Save domain config file"""
    ident = checkpointName
    if args.level == "diff":
        ident = int(time())
    configFile = f"{args.output}/vmconfig.{ident}.xml"
    logging.info("Saving VM config to: [%s]", configFile)
    try:
        with open(configFile, "w") as configFh:
            configFh.write(vmConfig)
        return configFile
    except OSError as errmsg:
        logging.error("Unable to save VM config: %s", errmsg)
        logging.exception(errmsg)
        sys.exit(1)


def setMetaContext(args, checkpointName, disk):
    """Set meta context passed to nbd server based on
    backup type"""
    metaContext = None
    if args.level in ("inc", "diff"):
        if args.offline is True:
            metaContext = f"qemu:dirty-bitmap:{checkpointName}"
        else:
            metaContext = f"qemu:dirty-bitmap:backup-{disk.diskTarget}"

        logging.info("INC/DIFF backup: set context to %s", metaContext)

    return metaContext


def setStreamType(args, disk):
    """Set target stream type"""
    if disk.diskFormat != "raw":
        streamType = args.type
    else:
        streamType = "raw"

    return streamType


def setTargetFile(args, disk, checkpointName):
    """Set Target file name to write"""
    if args.level in ("full", "copy"):
        if disk.diskFormat == "raw":
            level = "copy"
        else:
            level = args.level
        targetFile = f"{args.output}/{disk.diskTarget}.{level}.data"
    elif args.level in ("inc", "diff"):
        timestamp = int(time())
        # during diff backup, we do not create an checkpoint,
        # as such metadata header contains timestamp of
        # target file
        if args.level == "diff":
            checkpointName = timestamp
        targetFile = (
            f"{args.output}/"
            f"{disk.diskTarget}.{args.level}."
            f"{checkpointName}.data"
        )

    targetFilePartial = f"{targetFile}.partial"

    return targetFile, targetFilePartial


def getWriter(args, zipStream, targetFile, targetFilePartial):
    """Open target file based on output writer"""
    if args.stdout is True:
        targetFile = os.path.basename(targetFile)
        writer = zipStream.open(targetFile)
        logging.info("Write data to zip archive")

        return writer

    logging.info("Write data to target file: %s", targetFilePartial)
    try:
        writer = open(targetFilePartial, "wb")
    except OSError as e:
        raise exceptions.DiskBackupWriterException(
            f"Unable to open target file: {e}"
        ) from e

    return writer


def renamePartial(targetFilePartial, targetFile):
    """After backup, move .partial file to real
    target file"""
    try:
        os.rename(targetFilePartial, targetFile)
    except OSError as e:
        logging.error("Unable to rename file: %s", e)
        return False

    return True


def backupDisk(
    disk,
    count,
    args,
    lib,
    checkpointName,
    parentCheckpoint,
    backupSocket,
    zipStream,
):
    """Backup domain disk data."""

    stream = streamer.SparseStream(types)
    sTypes = types.SparseStreamTypes()

    current_thread().name = disk.diskTarget
    streamType = setStreamType(args, disk)
    metaContext = setMetaContext(args, checkpointName, disk)

    if args.offline is True:
        bitMap = None
        if args.level == "inc":
            bitMap = checkpointName
        backupSocket = f"{backupSocket}.{disk.diskTarget}"
        logging.info("Offline backup, starting NDB Service")
        err = qemuhelper.qemuHelper(disk.diskTarget).startBackupNbdServer(
            disk.diskFormat, disk.diskPath, backupSocket, bitMap
        )
        if err is not None:
            logging.fatal("Error starting NBD Server: %s", err)
            return False
        logging.info("NDB Service started")

    nbdClient = nbdhelper.nbdClient(disk.diskTarget, metaContext, backupSocket)
    connection = nbdClient.waitForServer()
    if not connection:
        raise exceptions.DiskBackupFailed(
            f"Error connecting to nbd endpoint: {backupSocket}"
        )

    if args.qemu:
        logging.info("Using qemu tools to query extents")
        extentHandler = extenthandler.ExtentHandler(
            qemuhelper.qemuHelper(disk.diskTarget), metaContext, backupSocket
        )
    else:
        extentHandler = extenthandler.ExtentHandler(
            connection, metaContext, backupSocket
        )
    extents = extentHandler.queryBlockStatus()
    diskSize = connection.get_size()

    if extents is None:
        logging.error("No extents found")
        return True

    thinBackupSize = sum([extent.length for extent in extents if extent.data is True])
    logging.info("Got %s extents to backup.", len(extents))
    logging.debug("%s", lib.dumpExtentJson(extents))
    logging.info("%s bytes disk size", diskSize)
    logging.info("%s bytes of data extents to backup", thinBackupSize)
    if args.printonly is True:
        nbdClient.disconnect()
        return True

    if args.level in ("inc", "diff") and thinBackupSize == 0:
        logging.info("No dirty blocks found")
        args.noprogress = True

    targetFile, targetFilePartial = setTargetFile(args, disk, checkpointName)
    writer = getWriter(args, zipStream, targetFile, targetFilePartial)

    if streamType == "raw":
        logging.info("Creating full provisioned raw backup image")
        try:
            writer.truncate(diskSize)
        except OSError as e:
            raise exceptions.DiskBackupWriterException(
                f"Unable to truncate target file: {e}"
            ) from e
        writer.seek(0)
    else:
        logging.info("Creating thin provisioned stream backup image")
        inc = args.level in ("inc", "diff")
        metadata = stream.dumpMetadata(
            diskSize,
            thinBackupSize,
            disk,
            checkpointName,
            parentCheckpoint,
            inc,
            args.compress,
        )
        stream.writeFrame(writer, sTypes.META, 0, len(metadata))
        writer.write(metadata)
        writer.write(sTypes.TERM)

    progressBar = lib.progressBar(
        thinBackupSize, f"saving disk {disk.diskTarget}", args, count=count
    )
    compressedSizes = []
    for save in extents:
        if save.data is True:
            if streamType == "stream":
                stream.writeFrame(writer, sTypes.DATA, save.offset, save.length)
                logging.debug(
                    "Read data from: start %s, length: %s", save.offset, save.length
                )

            cSizes = None

            if save.length >= nbdClient.maxRequestSize:
                logging.debug(
                    "Chunked data read from: start %s, length: %s",
                    save.offset,
                    save.length,
                )
                size, cSizes = lib.writeChunk(
                    writer,
                    save.offset,
                    save.length,
                    nbdClient.maxRequestSize,
                    connection,
                    streamType,
                    args.compress,
                )
            else:
                size = lib.writeBlock(
                    writer,
                    save.offset,
                    save.length,
                    connection,
                    streamType,
                    args.compress,
                )
                if streamType == "raw":
                    size = writer.seek(save.offset)

            if streamType == "stream":
                writer.write(sTypes.TERM)
                if args.compress is True:
                    logging.debug("Compressed size: %s", size)
                    if cSizes:
                        blockList = {}
                        blockList[size] = cSizes
                        compressedSizes.append(blockList)
                    else:
                        compressedSizes.append(size)
                else:
                    assert size == save.length

            progressBar.update(save.length)
        else:
            if streamType == "raw":
                writer.seek(save.offset)
            elif streamType == "stream" and args.level not in ("inc", "diff"):
                stream.writeFrame(writer, sTypes.ZERO, save.offset, save.length)
    if streamType == "stream":
        stream.writeFrame(writer, sTypes.STOP, 0, 0)
        if args.compress:
            stream.writeCompressionTrailer(writer, compressedSizes)

    progressBar.close()
    logging.debug("Closing write handle.")
    writer.close()
    nbdClient.disconnect()
    if args.offline is True:
        logging.info("Stopping NBD Service")
        lib.killNbdServer(backupSocket)
    if args.stdout is False:
        if args.noprogress is True:
            logging.info(
                "Backup of disk %s finished, file: %s", disk.diskTarget, targetFile
            )
        return renamePartial(targetFilePartial, targetFile)

    return True


if __name__ == "__main__":
    main()
